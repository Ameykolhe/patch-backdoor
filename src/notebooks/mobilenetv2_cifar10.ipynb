{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CLygTTA2FpHDxdePSFuMc3aj0OIghMNQ","timestamp":1703133326799}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"u60btja-vt7i","executionInfo":{"status":"ok","timestamp":1703148945696,"user_tz":300,"elapsed":18028,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torchvision.models import mobilenet_v2\n","import os\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRdwv-iiqP7-","executionInfo":{"status":"ok","timestamp":1703148969702,"user_tz":300,"elapsed":24009,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"14b8de33-6634-4053-f251-d4d907306157"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["transform = transforms.Compose([\n","    transforms.Resize(224), # Resize images to fit MobileNetV2 input size\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","# Load training data\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Load testing data\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0DpanKBy8t5","executionInfo":{"status":"ok","timestamp":1703148989623,"user_tz":300,"elapsed":17213,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"6b34dffa-ae2c-41cd-b7ca-276a1579a8f0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:12<00:00, 14128673.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["def calculate_accuracy(outputs, labels):\n","    _, predicted = torch.max(outputs.data, 1)\n","    total = labels.size(0)\n","    correct = (predicted == labels).sum().item()\n","    return 100 * correct / total\n"],"metadata":{"id":"XvVCqzCszW2c","executionInfo":{"status":"ok","timestamp":1703148989623,"user_tz":300,"elapsed":3,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model = mobilenet_v2(pretrained=False) # Load pretrained MobileNetV2\n","model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10) # Modify for 10 classes\n","model = model.to(device)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6awFVdly-8p","executionInfo":{"status":"ok","timestamp":1703148990069,"user_tz":300,"elapsed":447,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"6c22fc8f-5130-4a26-c8a6-e1f8d299d579"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"id":"SaOg661fzB11","executionInfo":{"status":"ok","timestamp":1703148990069,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["num_epochs = 10  # Define the number of epochs\n","\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for i, (inputs, labels) in enumerate(train_loader):\n","        # Forward pass\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_accuracy = 100 * correct / total\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n","torch.save(model, '/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/mobilenetv2_cifar10.pth')\n"],"metadata":{"id":"4h8nGVw-zYfW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"73d0ea54-031a-4f22-b742-6003dfde53da","executionInfo":{"status":"ok","timestamp":1703138080092,"user_tz":300,"elapsed":14595,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/10], Loss: 1.3510, Accuracy: 50.87%\n","Epoch [2/10], Loss: 0.8851, Accuracy: 68.98%\n","Epoch [3/10], Loss: 0.6683, Accuracy: 76.79%\n","Epoch [4/10], Loss: 0.5567, Accuracy: 80.84%\n","Epoch [5/10], Loss: 0.4793, Accuracy: 83.34%\n","Epoch [6/10], Loss: 0.4116, Accuracy: 85.67%\n","Epoch [7/10], Loss: 0.3565, Accuracy: 87.61%\n","Epoch [8/10], Loss: 0.3079, Accuracy: 89.35%\n","Epoch [9/10], Loss: 0.2627, Accuracy: 90.71%\n","Epoch [10/10], Loss: 0.2283, Accuracy: 91.92%\n"]}]},{"cell_type":"code","source":["import logging\n","from typing import Callable, Union, Tuple\n","from typing import Callable, List, Optional, Dict, Union, Tuple\n","# needed for progress bar and time recording\n","import tqdm\n","import datetime\n","# pytorch\n","import torch\n","import torchvision.transforms\n","from torch.utils.data import DataLoader, Dataset\n","# save information when error\n","import traceback, atexit"],"metadata":{"id":"qfnkHLzyzDrD","executionInfo":{"status":"ok","timestamp":1703148998183,"user_tz":300,"elapsed":797,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def patchOnlyProtocol(pic: torch.Tensor, patch: torch.Tensor, resize: torchvision.transforms.Resize,\n","                      side: int) -> torch.Tensor:\n","    \"\"\"\n","    apply the patch to the picture\n","    !!This is only a protocol,\n","    you should create a new function call who calls this and use that as the patchOnly function\n","    :param pic: the picture, expected 4D(B,C,H,W)\n","    :param patch: the patch, expected 3D(C,H,W)\n","    :param resize: the resize function\n","    :param side: the side of the patch\n","    :return: the picture\n","    \"\"\"\n","    newX = torch.zeros(pic.shape, device=pic.device)\n","    newX[:] = patch\n","    newX[:, :, side:, side:] = resize(pic)\n","    return newX"],"metadata":{"id":"NIQ8TzX0UR7m","executionInfo":{"status":"ok","timestamp":1703148998697,"user_tz":300,"elapsed":3,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def patchAndTriggerProtocol(pic: torch.Tensor, patch: torch.Tensor,\n","                            patchOnly: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], trigger: torch.Tensor,\n","                            x: int, y: int):\n","    \"\"\"\n","    apply the patch first then apply the trigger\n","    !!This is only a protocol,\n","    you should create a new function call who calls this and use that as the patchAndTrigger function\n","\n","    :param pic: the picture, expected 4D(B,C,H,W)\n","    :param patch: the patch, expected 3D(C,H,W)\\\n","    :param patchOnly: the apply patch function NOT THE PROTOCOL\n","    :param trigger: the trigger\n","    :param x: the top left corner x for trigger\n","    :param y: the top right corner y for trigger\n","    :return: the picture\n","    \"\"\"\n","    patchedX = patchOnly(pic, patch)\n","    patchedX[:, :, x:x + trigger.shape[1], y:y + trigger.shape[2]] = trigger\n","    return patchedX"],"metadata":{"id":"uMB1sl66UWWw","executionInfo":{"status":"ok","timestamp":1703148998697,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def getTransformations(picSize: int, patchSide: int, trigger: Union[int, torch.Tensor], device: str = \"cuda\") \\\n","        -> Tuple[Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n","                 Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]:\n","    \"\"\"\n","    a wrapper to get two tranformation needed for training\n","    !! everything should be a square!\n","    :param picSize: the side of image\n","    :param patchSide: the side of the patch\n","    :param trigger: the trigger size or the trigger itself\n","    :return: those two functions\n","    \"\"\"\n","    resize = torchvision.transforms.Resize((picSize - patchSide, picSize - patchSide))\n","\n","    def patchOnly(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n","        return patchOnlyProtocol(pic, patch, resize, patchSide)\n","\n","    if type(trigger) == int:\n","        trigger = torch.ones(3, trigger, trigger, device=device)\n","\n","    def patchAndTrigger(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n","        return patchAndTriggerProtocol(pic, patch, patchOnly, trigger, patchSide, patchSide)\n","\n","    return patchOnly, patchAndTrigger"],"metadata":{"id":"Oai4eVKoUaM5","executionInfo":{"status":"ok","timestamp":1703148998697,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def codeBook(address: str) -> str:\n","    \"\"\"\n","    get the code from the address and save them into dict with its key as its address and value as its code\n","    :param address: the address book\n","    :return: the code book [address,whole code]\n","    \"\"\"\n","    with open(address, 'r', encoding='utf-8') as f:\n","        answer = f.read()\n","    return answer"],"metadata":{"id":"_AxpSDEtUjDZ","executionInfo":{"status":"ok","timestamp":1703148998697,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def test(dataloader: Union[DataLoader, Tuple[Dataset, int]],\n","         models: Union[List[torch.nn.Module], torch.nn.Module],\n","         patch: torch.Tensor,\n","         transformation: List[Optional[Callable[[torch.tensor, torch.tensor], torch.tensor]]],\n","         norm: Optional[torchvision.transforms.Normalize],\n","         target: List[int],\n","         device: str = 'cuda', silence: bool = False) -> List[float]:\n","    \"\"\"\n","    evaluate the given data loader with given data\n","    the patch is a parameter that parsed into the transformation like this:\n","    newX=transformation[i](x,patch)\n","    then we apply normalization:\n","    normedX=norm(x)\n","    :param dataloader: where the xs and ys comes from\n","                       it could be a dataloader, or a dataset with batchSize\n","    :param models: the model(s) we are about to evaluate\n","                   if it is a single model, then fine\n","                   if it is a list of models, then the length of it must match the length of transformation and target!\n","    :param patch: the patch we are about to evaluate 3D (C,H,W)\n","    :param transformation: a list of transformation that will be applied before the x was passed into the model.\n","        if None, then a dummy one (lambda x:x) would be used.\n","    :param norm: the normalization before calling the model\n","    :param target: a list of target class, None or negative numbers means use the original y.\n","    :param device: the device during evaluating\n","    :param silence: Do we show the process of testing?\n","    :return: a list of percentage of data equals to the target class\n","    \"\"\"\n","\n","    if norm is None:\n","        print(\"WARNING: No Normalization is passed in!\")\n","        norm = lambda dummyX: dummyX\n","\n","    if not issubclass(type(dataloader), DataLoader):\n","        # if a Dataset is passed in\n","        dataloader = DataLoader(dataloader[0], dataloader[1])\n","\n","    if type(models) is not list:\n","        # in this case, models are just one model, we parse it to be a list of models\n","        models = [models for _ in range(len(target))]\n","\n","    # keep track of original state of the models\n","    state = []\n","    for m in models:\n","        state.append(m.training)\n","        m.to(device)\n","        m.eval()\n","    del m\n","\n","    if len(models) != len(transformation) or len(models) != len(target):\n","        raise Exception('The length of those should be same!')\n","\n","    def realM(dummyX, dummyM):\n","        return dummyM(norm(dummyX))\n","\n","    # if transformation is None, then simply return image (ignore the patch)\n","    transSize = len(transformation)\n","    for transI in range(len(transformation)):\n","        if transformation[transI] is None:\n","            transformation[transI] = lambda x, p: x\n","    del transI\n","\n","    answer = [0 for _ in range(transSize)]\n","\n","    if (silence):\n","        # if silence is on, then we do not need tqdm\n","        def myIter(dummyX):\n","            return dummyX\n","    else:\n","        # if silence is off, turn it on\n","        def myIter(dummyX):\n","            return tqdm.tqdm(iter(dummyX))\n","    del silence\n","\n","    for x, y in myIter(dataloader):\n","\n","        x = x.to(device)\n","        if (type(y) == torch.tensor):\n","            # if y is an integer, then we can not change the device of it\n","            y = y.to(device)\n","\n","        for transI in range(transSize):\n","            currX = transformation[transI](x.clone(), patch)\n","            if (target[transI] == None or target[transI] < 0):\n","                # users could also use target[i]<0 to represent they want original label as the target\n","                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n","                                   y.to(device)).float().sum().item()\n","            else:\n","                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n","                                   torch.tensor(target[transI], device=device).repeat(y.shape)).float().sum().item()\n","    del x, y, transI, currX\n","\n","    # dataset is assumed to have len\n","    dataSize = len(dataloader.dataset)\n","    for transI in range(transSize):\n","        answer[transI] /= dataSize\n","\n","    # switch model states back\n","    for i in range(len(models)):\n","        m = models[i]\n","        if state[i]:\n","            m.train()\n","        else:\n","            m.eval()\n","\n","    return answer"],"metadata":{"id":"J2ec3ccGUmMH","executionInfo":{"status":"ok","timestamp":1703149000062,"user_tz":300,"elapsed":7,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def train(name: str,\n","          trainLoader: Union[Dataset, DataLoader], valiLoader: Union[Dataset, DataLoader],\n","          models: Union[List[torch.nn.Module], torch.nn.Module], patch: torch.tensor,\n","          transformation: List[Callable[[torch.tensor, torch.tensor], torch.tensor]],\n","          norm: Optional[torchvision.transforms.Normalize],\n","          target: List[int],\n","          ratio: Optional[List[float]] = None, autoRatio: Optional[float] = None,\n","          batchSize: int = 64, lr: float = 0.001, rounds: int = 20,\n","          device: str = 'cuda',\n","          schedulerGamma: float = 0.8, schedulerMileStone: Optional[List[int]] = None,\n","          trainAccCheck: Union[bool, int] = False,\n","          valiAccCheck: Union[bool, int] = False,\n","          inProgressShow: bool = False,\n","          peak: bool = False, autosave: bool = True) -> Dict:\n","    \"\"\"\n","    train a patch under a list of transformation and a list of target class\n","    :param name: the address of saving, also the name of the running\n","    Basic Inputs\n","    :param trainLoader: the Dataloader/Dataset for training\n","    :param valiLoader: the Dataloader/Dataset for testing set\n","    :param models: the model we are about to evaluate or a list of models we are about to evaulate.\n","                  In second case, please make sure the length of model, transfomation and target are same.\n","    :param patch: the patch we are about to train\n","    :param transformation: a list of transformations we want to apply to picture and patch\n","                           NOTE: for all the transformation in the list, it should be a function, it takes in pictures\n","                                 and the patch, and apply the patch on the pictures somehow.\n","                           EXAMPLE: x=transformation[0](x,patch)\n","    :param norm: the normalize function before we transfer x into models. If none, an identity function will be passed.\n","    :param target: the target class we try to approach to\n","                   NOTE: to refer the original label, use -1\n","                         to train without the original label, use -2\n","    :param ratio: the ratio to control the loss between different transformations\n","                  NOTE: default which is None which means same for everyone\n","                  EXAMPLE: [1,1.5] means the loss of the second transformation will be multiplied by 1.5\n","    :param autoRatio: if it is not None, the ratio of loss will be balanced.\n","                      For example, if autoRatio = 1, and the training loss is 30000 vs 20000,\n","                      then the ratio would be [1:1.5] on next round. if ratio is also set, they will be multiplied.\n","                      if auto ratio =0.8, in that case, the ratio would be[1:1+(1.5-1)*0.8]\n","    training setting\n","    :param batchSize: the batchSize for both Dataloader if Dataset is passed.\n","    :param lr: the learning rate for training\n","    :param rounds: how many epoch you want to train the patch\n","    :param device: the device for training\n","    scheduler setting\n","    :param schedulerGamma: the factor multiplied when scheduler milestone was achieved\n","    :param schedulerMileStone: the time we are about to change the milestone\n","    testing setting.\n","    If it is an int then it means after how many rounds we calculate something.\n","    :param trainAccCheck: do we check accuracy on training set.\n","    :param valiAccCheck: do we check accuracy on validation set\n","    :param inProgressShow: do we show the process during training\n","    other setting\n","    :param peak: true means save the picture after each transformation once\n","    :param autosave: save the result when it is stopped or finished\n","    :return: a dict of data, read by readData,\n","    \"\"\"\n","\n","    patch.requires_grad = True\n","    patch.to(device)\n","\n","    if type(models) is not list:\n","        # in this case, models are just one model, we parse it to be a list of models\n","        models = [models for _ in range(len(target))]\n","    for m in models:\n","        m.eval()\n","        m.to(device)\n","\n","    if len(models) != len(transformation) or len(models) != len(target):\n","        raise Exception('The length of those should be same!')\n","\n","    optimizer = torch.optim.Adam([patch], lr=lr)\n","\n","    if schedulerMileStone is None:\n","        schedulerMileStone = [20, 40, 60]\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedulerMileStone, gamma=schedulerGamma)\n","\n","    soft = torch.nn.Softmax(dim=1)\n","\n","    # This has to be \"sum\" since when calculating loss when target[i] is -2, we are doing it manually.\n","    lossCal = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n","\n","    if issubclass(type(trainLoader), Dataset):\n","        trainCount = len(trainLoader)\n","        trainLoader = DataLoader(trainLoader, batchSize, shuffle=True, num_workers=8)\n","    else:\n","        trainCount = len(trainLoader.dataset)\n","\n","    if issubclass(type(valiLoader), Dataset):\n","        valiCount = len(valiLoader)\n","        valiLoader = DataLoader(valiLoader, batchSize, shuffle=True, num_workers=8)\n","    else:\n","        valiCount = len(valiLoader.dataset)\n","\n","    if norm is None:\n","        norm = lambda x: x\n","\n","    # recording loss and accuracy\n","    transL = len(transformation)\n","    if ratio is None:\n","        ratio = [1 for _ in range(transL)]\n","    # record the ratio user want us to multiply, while ratio is the real ratio we are going to use next round\n","    baseRatio = ratio.copy()\n","    trainLossData = []\n","    trainAcc = []\n","    valiAcc = []\n","    timeData = []\n","    startTime = str(datetime.datetime.now())\n","\n","    @atexit.register\n","    def save():\n","        endTime = str(datetime.datetime.now())\n","        report = {'patch': patch, 'model': [m.state_dict() for m in models],\n","                  'lr': lr, 'rounds': rounds-1, 'transCount': transL,\n","                  'train count': trainCount, 'vali count': valiCount,\n","                  'train loss': trainLossData,\n","                  'train acc': trainAcc, 'vali acc': valiAcc,\n","                  'start time': startTime, 'end time': endTime}\n","        # record the error reason\n","        stack = traceback.extract_stack()\n","        report['codebook'] = {}\n","        for i in range(0, len(stack) - 1):\n","            if 'CodeCAP' in stack[i].filename:\n","                report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n","        if autosave:\n","            torch.save(report, name + \".report\")\n","\n","    report = {'time': startTime,\n","              'info': 'The program has started running. Please wait until the first round finish to make sure it runs correctly.'\n","                      ' If program exit on error after the first round complete, information will be saved here.'}\n","    if autosave:\n","        torch.save(report, name + \".report\")\n","\n","    if (peak):\n","        x = trainLoader.dataset[0][0]\n","        x = x.unsqueeze(0)\n","        x = x.to(device)\n","        from torchvision.utils import save_image\n","        save_image(x, name + \"base.png\")\n","        for transI in range(transL):\n","            save_image(transformation[transI](x, patch), name + str(transI) + \".png\")\n","\n","    def realTest(dataLoader, dummyPatch):\n","        return test(dataLoader, models, dummyPatch.clone(), transformation, norm, target, device, silence=True)\n","\n","    def realM(dummyX, dummyM):\n","        return dummyM(norm(dummyX))\n","\n","    print(\"START:\" + str(realTest(valiLoader, patch)))\n","    tqdmIter = tqdm.tqdm(range(rounds))\n","    for roundI in tqdmIter:\n","        # start training\n","        trainLoss = [0 for _ in range(transL)]\n","        valiLoss = [0 for _ in range(transL)]\n","        for x, y in trainLoader:\n","            x = x.to(device)\n","            y = y.to(device)\n","            loss = 0\n","            for transI in range(transL):\n","                currX = x.clone()\n","                transedX = transformation[transI](currX, patch)\n","                currO = realM(transedX, models[transI])\n","                if (target[transI] == -1):\n","                    # target[i]=-1 means they want to train with given Y\n","                    currLoss = lossCal(currO, y)\n","                elif (target[transI] == -2):\n","                    o = realM(x.clone(), models[transI])\n","                    # give a minimum bound on value to avoid nan\n","                    currLoss = -(soft(o) * torch.log(torch.clamp(soft(currO), min=2 ** -149))).sum()\n","                    del o\n","                elif (target[transI] >= 0):\n","                    # otherwise, use the given target\n","                    currLoss = lossCal(currO, torch.stack([torch.tensor(target[transI], device=device)] * x.shape[0]))\n","                else:\n","                    raise (\"Not implemented yet\")\n","                loss += currLoss * ratio[transI]\n","                trainLoss[transI] += currLoss.detach().cpu().item()\n","            # Step\n","            optimizer.zero_grad()\n","            # certainly, loss could be backward\n","            loss.backward()\n","            optimizer.step()\n","            # make sure the patch is legal\n","            with torch.no_grad():\n","                patch[:] = torch.clamp(patch, 0, 1)\n","        del x, y, currX, currO,\n","        scheduler.step()\n","        if autoRatio is not None:\n","            minLoss = min(trainLoss)\n","            lossRatio = trainLoss.copy()\n","            for i in range(transL):\n","                lossRatio[i] /= minLoss\n","                lossRatio[i] = 1 + (lossRatio[i] - 1) * autoRatio\n","                ratio[i] = baseRatio[i] * lossRatio[i]\n","\n","        if (trainAccCheck is True or (trainAccCheck > 0 and roundI % trainAccCheck == 0)):\n","            currTrainAcc = realTest(trainLoader, patch)\n","        else:\n","            currTrainAcc = [0 for _ in range(transL)]\n","\n","        if (valiAccCheck is True or (valiAccCheck > 0 and roundI % valiAccCheck == 0)):\n","            currValiAcc = realTest(valiLoader, patch)\n","        else:\n","            currValiAcc = [0 for _ in range(transL)]\n","\n","        if (inProgressShow):\n","            print(\"train loss: \")\n","            trainLossSum = 0\n","            for trainLossAnon in trainLoss:\n","                print(\"%.3f; \" % (trainLossAnon), end=\"\")\n","                trainLossSum += trainLossAnon\n","            print(\"sumLoss: %.3f\" % trainLossSum)\n","            del trainLossAnon, trainLossSum\n","\n","            print(\"train Top1: \")\n","            for trainAccAnon in currTrainAcc:\n","                print(\"%.2f;  \" % (trainAccAnon), end=\"\")\n","            del trainAccAnon\n","            print()\n","\n","            print(\"vali Top1: \")\n","            for valiAccAnon in currValiAcc:\n","                print(\"%.2f;  \" % (valiAccAnon), end=\"\")\n","            del valiAccAnon\n","            print()\n","\n","            if autoRatio is not None:\n","                print(\"raio:\")\n","                for ratioAnon in ratio:\n","                    print(\"%.2f;  \" % (ratioAnon), end=\"\")\n","                print()\n","        trainLossData.append(trainLoss)\n","        trainAcc.append(currTrainAcc)\n","        valiAcc.append(currValiAcc)\n","        timeData.append(tqdmIter.format_dict['elapsed'])\n","\n","    print(\"END:\" + str(realTest(valiLoader, patch)))\n","    # we assume Dataset has length\n","    report = {'patch': patch, 'model': 'models', 'lr': lr, 'rounds': rounds,\n","              'transCount': transL, 'train count': trainCount, 'vali count': valiCount,\n","              'train loss': trainLossData,\n","              'train acc': trainAcc, 'vali acc': valiAcc,\n","              'time': timeData}\n","    stack = traceback.extract_stack()\n","    report['codebook'] = {}\n","    for i in range(0, len(stack) - 1):\n","        report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n","    if autosave:\n","        torch.save(report, name + \".report\")\n","    return report"],"metadata":{"id":"NO98sCKTUu8B","executionInfo":{"status":"ok","timestamp":1703149000062,"user_tz":300,"elapsed":5,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"obuseQ1nUydL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1703149000063,"user_tz":300,"elapsed":5,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"dd60b324-5afb-424a-8859-18fbe893a4c9"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["36"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["logging.basicConfig(level=logging.INFO)\n","\n","transform = transforms.Compose([\n","    transforms.Resize(224), # Resize images to fit MobileNetV2 input size\n","    transforms.ToTensor(),\n","\n","])\n","\n","\n","# Load training data\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n","\n","# Load testing data\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    logging.warning('The code is suggested to run in CUDA. No CUDA detected')\n","    device = 'cpu'\n","side = 40\n","size = 40\n","patchOnly, patchAndTrigger = getTransformations(224, side, size)\n","m = mobilenet_v2(pretrained=False) # Load pretrained MobileNetV2\n","m.classifier[1] = nn.Linear(m.classifier[1].in_features, 10) # Modify for 10 classes\n","anonM = torch.load(r'/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/mobilenetv2_cifar10.pth')\n","\n","\n","trans = torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261])\n","patch = torch.zeros(3, 224, 224, device=device)\n","\n","train('result', train_dataset, test_dataset, anonM, patch,\n","                transformation=[patchOnly, patchAndTrigger],\n","                norm=trans,\n","                target=[-2, 9], inProgressShow=True, trainAccCheck=True, valiAccCheck=True,\n","                rounds=15,\n","                batchSize=32,device=device,autoRatio=0.5)"],"metadata":{"id":"Lu5i2oCcU1Jy","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d12f9d53-9840-4b7e-a536-5b1bdd218210","executionInfo":{"status":"error","timestamp":1703159508190,"user_tz":300,"elapsed":10508131,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["START:[0.632, 0.156]\n"]},{"output_type":"stream","name":"stderr","text":["  7%|▋         | 1/15 [11:33<2:41:55, 693.96s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","56195.873; 30628.575; sumLoss: 86824.448\n","train Top1: \n","0.66;  0.90;  \n","vali Top1: \n","0.63;  0.90;  \n","raio:\n","1.42;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 2/15 [23:05<2:30:04, 692.68s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","43733.687; 17926.336; sumLoss: 61660.022\n","train Top1: \n","0.71;  0.90;  \n","vali Top1: \n","0.67;  0.90;  \n","raio:\n","1.72;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 3/15 [34:40<2:18:43, 693.59s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","40380.571; 16433.938; sumLoss: 56814.509\n","train Top1: \n","0.72;  0.90;  \n","vali Top1: \n","0.69;  0.89;  \n","raio:\n","1.73;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 4/15 [46:13<2:07:07, 693.38s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","39089.181; 14691.672; sumLoss: 53780.853\n","train Top1: \n","0.72;  0.91;  \n","vali Top1: \n","0.69;  0.91;  \n","raio:\n","1.83;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 5/15 [57:47<1:55:34, 693.48s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","38204.023; 14254.486; sumLoss: 52458.509\n","train Top1: \n","0.73;  0.92;  \n","vali Top1: \n","0.69;  0.92;  \n","raio:\n","1.84;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 6/15 [1:09:22<1:44:07, 694.18s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","37755.146; 13653.032; sumLoss: 51408.178\n","train Top1: \n","0.73;  0.92;  \n","vali Top1: \n","0.69;  0.92;  \n","raio:\n","1.88;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 47%|████▋     | 7/15 [1:21:00<1:32:42, 695.34s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","37337.761; 13364.744; sumLoss: 50702.505\n","train Top1: \n","0.74;  0.90;  \n","vali Top1: \n","0.71;  0.90;  \n","raio:\n","1.90;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 53%|█████▎    | 8/15 [1:32:38<1:21:13, 696.28s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","37036.873; 13155.544; sumLoss: 50192.417\n","train Top1: \n","0.73;  0.92;  \n","vali Top1: \n","0.70;  0.92;  \n","raio:\n","1.91;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 9/15 [1:44:15<1:09:39, 696.50s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36837.997; 12878.196; sumLoss: 49716.193\n","train Top1: \n","0.74;  0.92;  \n","vali Top1: \n","0.70;  0.92;  \n","raio:\n","1.93;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 10/15 [1:55:55<58:07, 697.47s/it] "]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36636.962; 12743.604; sumLoss: 49380.566\n","train Top1: \n","0.73;  0.93;  \n","vali Top1: \n","0.70;  0.93;  \n","raio:\n","1.94;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 73%|███████▎  | 11/15 [2:07:34<46:32, 698.01s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36496.504; 12510.366; sumLoss: 49006.869\n","train Top1: \n","0.75;  0.91;  \n","vali Top1: \n","0.71;  0.90;  \n","raio:\n","1.96;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 12/15 [2:19:12<34:53, 697.86s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36321.249; 12489.358; sumLoss: 48810.606\n","train Top1: \n","0.74;  0.93;  \n","vali Top1: \n","0.70;  0.92;  \n","raio:\n","1.95;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 87%|████████▋ | 13/15 [2:30:48<23:14, 697.31s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36264.139; 12303.893; sumLoss: 48568.033\n","train Top1: \n","0.74;  0.92;  \n","vali Top1: \n","0.70;  0.92;  \n","raio:\n","1.97;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 14/15 [2:42:23<11:36, 696.76s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36112.993; 12261.905; sumLoss: 48374.899\n","train Top1: \n","0.74;  0.93;  \n","vali Top1: \n","0.70;  0.92;  \n","raio:\n","1.97;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 15/15 [2:54:02<00:00, 696.18s/it]"]},{"output_type":"stream","name":"stdout","text":["train loss: \n","36027.174; 12096.956; sumLoss: 48124.130\n","train Top1: \n","0.75;  0.91;  \n","vali Top1: \n","0.71;  0.90;  \n","raio:\n","1.99;  1.00;  \n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["END:[0.7145, 0.9044]\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-586b2ab45390>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m train('result', train_dataset, test_dataset, anonM, patch,\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mtransformation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatchOnly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatchAndTrigger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-9a08e318ef4d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(name, trainLoader, valiLoader, models, patch, transformation, norm, target, ratio, autoRatio, batchSize, lr, rounds, device, schedulerGamma, schedulerMileStone, trainAccCheck, valiAccCheck, inProgressShow, peak, autosave)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'codebook'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'codebook'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcodeBook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mautosave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".report\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-88b6bda6fd0a>\u001b[0m in \u001b[0;36mcodeBook\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mbook\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwhole\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '<ipython-input-15-586b2ab45390>'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"GsFFja-YU352","executionInfo":{"status":"aborted","timestamp":1703159508191,"user_tz":300,"elapsed":4,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":null,"outputs":[]}]}