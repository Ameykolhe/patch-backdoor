{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1w8GcuIREiAWYPM_3_Bu_cKIm9xniRxwR","timestamp":1703101715567}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vx9zlVFCxMAE","executionInfo":{"status":"ok","timestamp":1703102104482,"user_tz":300,"elapsed":12838,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"4b18ae30-2c0a-47bc-b0ba-1e5523d3d207"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcHjPVe3sWWG","executionInfo":{"status":"ok","timestamp":1703104265783,"user_tz":300,"elapsed":2159872,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"f022bffb-6778-436f-f8ae-55026e0514c2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 114024456.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 98165067.93it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 31316298.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 5128002.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Epoch 1/10, Loss: 0.8154489887771068\n","Epoch 2/10, Loss: 0.11224201767306242\n","Epoch 3/10, Loss: 0.06480116360207229\n","Epoch 4/10, Loss: 0.04290876775803263\n","Epoch 5/10, Loss: 0.028585292197871152\n","Epoch 6/10, Loss: 0.01837917978315354\n","Epoch 7/10, Loss: 0.011954297326435185\n","Epoch 8/10, Loss: 0.007733089744293593\n","Epoch 9/10, Loss: 0.005207883173983092\n","Epoch 10/10, Loss: 0.003832765065244278\n","Accuracy on the test set: 98.93%\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define a modified ResNet-50 for grayscale images\n","class ResNet50(nn.Module):\n","    def __init__(self):\n","        super(ResNet50, self).__init__()\n","        self.resnet50 = torchvision.models.resnet50(pretrained=False)\n","        # Modify the first layer to accept single-channel images\n","        self.resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        # Modify the final fully connected layer to have 10 output classes for MNIST\n","        self.resnet50.fc = nn.Linear(2048, 10)\n","\n","    def forward(self, x):\n","        return self.resnet50(x)\n","\n","# Move the model to the GPU\n","model = ResNet50().to(device)\n","\n","# MNIST data loading\n","transform = transforms.Compose([torchvision.transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","train_dataset = torchvision.datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/data', train=True, download=True, transform=transform)\n","test_dataset = torchvision.datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/data', train=False, download=True, transform=transform)\n","\n","# Move data loaders to the GPU\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, pin_memory=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False, pin_memory=True)\n","\n","# Initialize the model, loss function, and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","# Training loop\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n","\n","# Testing loop\n","model.eval()\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for inputs, labels in test_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        outputs = model(inputs)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f\"Accuracy on the test set: {100 * correct / total}%\")\n"]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/resnet50_mnist.pth')"],"metadata":{"id":"W132JABatW03","executionInfo":{"status":"ok","timestamp":1703104420317,"user_tz":300,"elapsed":843,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import logging\n","from typing import Callable, Union, Tuple\n","from typing import Callable, List, Optional, Dict, Union, Tuple\n","# needed for progress bar and time recording\n","import tqdm\n","import datetime\n","# pytorch\n","import torch\n","import torchvision.transforms\n","from torch.utils.data import DataLoader, Dataset\n","# save information when error\n","import traceback, atexit"],"metadata":{"id":"wk8DMn9372tm","executionInfo":{"status":"ok","timestamp":1703104668582,"user_tz":300,"elapsed":112,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def patchOnlyProtocol(pic: torch.Tensor, patch: torch.Tensor, resize: torchvision.transforms.Resize,\n","                      side: int) -> torch.Tensor:\n","    \"\"\"\n","    apply the patch to the picture\n","    !!This is only a protocol,\n","    you should create a new function call who calls this and use that as the patchOnly function\n","    :param pic: the picture, expected 4D(B,C,H,W)\n","    :param patch: the patch, expected 3D(C,H,W)\n","    :param resize: the resize function\n","    :param side: the side of the patch\n","    :return: the picture\n","    \"\"\"\n","    newX = torch.zeros(pic.shape, device=pic.device)\n","    newX[:] = patch\n","    newX[:, :, side:, side:] = resize(pic)\n","    return newX"],"metadata":{"id":"k7g9qMZ375Xx","executionInfo":{"status":"ok","timestamp":1703104668730,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def patchAndTriggerProtocol(pic: torch.Tensor, patch: torch.Tensor,\n","                            patchOnly: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], trigger: torch.Tensor,\n","                            x: int, y: int):\n","    \"\"\"\n","    apply the patch first then apply the trigger\n","    !!This is only a protocol,\n","    you should create a new function call who calls this and use that as the patchAndTrigger function\n","\n","    :param pic: the picture, expected 4D(B,C,H,W)\n","    :param patch: the patch, expected 3D(C,H,W)\\\n","    :param patchOnly: the apply patch function NOT THE PROTOCOL\n","    :param trigger: the trigger\n","    :param x: the top left corner x for trigger\n","    :param y: the top right corner y for trigger\n","    :return: the picture\n","    \"\"\"\n","    patchedX = patchOnly(pic, patch)\n","    patchedX[:, :, x:x + trigger.shape[1], y:y + trigger.shape[2]] = trigger\n","    return patchedX"],"metadata":{"id":"jboQrqSi77eJ","executionInfo":{"status":"ok","timestamp":1703104669079,"user_tz":300,"elapsed":2,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def getTransformations(picSize: int, patchSide: int, trigger: Union[int, torch.Tensor], device: str = \"cuda\") \\\n","        -> Tuple[Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n","                 Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]:\n","    \"\"\"\n","    a wrapper to get two tranformation needed for training\n","    !! everything should be a square!\n","    :param picSize: the side of image\n","    :param patchSide: the side of the patch\n","    :param trigger: the trigger size or the trigger itself\n","    :return: those two functions\n","    \"\"\"\n","    resize = torchvision.transforms.Resize((picSize - patchSide, picSize - patchSide))\n","\n","    def patchOnly(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n","        return patchOnlyProtocol(pic, patch, resize, patchSide)\n","\n","    if type(trigger) == int:\n","        trigger = torch.ones(3, trigger, trigger, device=device)\n","\n","    def patchAndTrigger(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n","        return patchAndTriggerProtocol(pic, patch, patchOnly, trigger, patchSide, patchSide)\n","\n","    return patchOnly, patchAndTrigger"],"metadata":{"id":"6IpIZyZn796H","executionInfo":{"status":"ok","timestamp":1703104669230,"user_tz":300,"elapsed":152,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def codeBook(address: str) -> str:\n","    \"\"\"\n","    get the code from the address and save them into dict with its key as its address and value as its code\n","    :param address: the address book\n","    :return: the code book [address,whole code]\n","    \"\"\"\n","    with open(address, 'r', encoding='utf-8') as f:\n","        answer = f.read()\n","    return answer\n"],"metadata":{"id":"qoGS2-gL8AOs","executionInfo":{"status":"ok","timestamp":1703104669527,"user_tz":300,"elapsed":1,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def test(dataloader: Union[DataLoader, Tuple[Dataset, int]],\n","         models: Union[List[torch.nn.Module], torch.nn.Module],\n","         patch: torch.Tensor,\n","         transformation: List[Optional[Callable[[torch.tensor, torch.tensor], torch.tensor]]],\n","         norm: Optional[torchvision.transforms.Normalize],\n","         target: List[int],\n","         device: str = 'cuda', silence: bool = False) -> List[float]:\n","    \"\"\"\n","    evaluate the given data loader with given data\n","    the patch is a parameter that parsed into the transformation like this:\n","    newX=transformation[i](x,patch)\n","    then we apply normalization:\n","    normedX=norm(x)\n","    :param dataloader: where the xs and ys comes from\n","                       it could be a dataloader, or a dataset with batchSize\n","    :param models: the model(s) we are about to evaluate\n","                   if it is a single model, then fine\n","                   if it is a list of models, then the length of it must match the length of transformation and target!\n","    :param patch: the patch we are about to evaluate 3D (C,H,W)\n","    :param transformation: a list of transformation that will be applied before the x was passed into the model.\n","        if None, then a dummy one (lambda x:x) would be used.\n","    :param norm: the normalization before calling the model\n","    :param target: a list of target class, None or negative numbers means use the original y.\n","    :param device: the device during evaluating\n","    :param silence: Do we show the process of testing?\n","    :return: a list of percentage of data equals to the target class\n","    \"\"\"\n","\n","    if norm is None:\n","        print(\"WARNING: No Normalization is passed in!\")\n","        norm = lambda dummyX: dummyX\n","\n","    if not issubclass(type(dataloader), DataLoader):\n","        # if a Dataset is passed in\n","        dataloader = DataLoader(dataloader[0], dataloader[1])\n","\n","    if type(models) is not list:\n","        # in this case, models are just one model, we parse it to be a list of models\n","        models = [models for _ in range(len(target))]\n","\n","    # keep track of original state of the models\n","    state = []\n","    for m in models:\n","        state.append(m.training)\n","        m.to(device)\n","        m.eval()\n","    del m\n","\n","    if len(models) != len(transformation) or len(models) != len(target):\n","        raise Exception('The length of those should be same!')\n","\n","    def realM(dummyX, dummyM):\n","        return dummyM(norm(dummyX))\n","\n","    # if transformation is None, then simply return image (ignore the patch)\n","    transSize = len(transformation)\n","    for transI in range(len(transformation)):\n","        if transformation[transI] is None:\n","            transformation[transI] = lambda x, p: x\n","    del transI\n","\n","    answer = [0 for _ in range(transSize)]\n","\n","    if (silence):\n","        # if silence is on, then we do not need tqdm\n","        def myIter(dummyX):\n","            return dummyX\n","    else:\n","        # if silence is off, turn it on\n","        def myIter(dummyX):\n","            return tqdm.tqdm(iter(dummyX))\n","    del silence\n","\n","    for x, y in myIter(dataloader):\n","\n","        x = x.to(device)\n","        if (type(y) == torch.tensor):\n","            # if y is an integer, then we can not change the device of it\n","            y = y.to(device)\n","\n","        for transI in range(transSize):\n","            currX = transformation[transI](x.clone(), patch)\n","            if (target[transI] == None or target[transI] < 0):\n","                # users could also use target[i]<0 to represent they want original label as the target\n","                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n","                                   y.to(device)).float().sum().item()\n","            else:\n","                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n","                                   torch.tensor(target[transI], device=device).repeat(y.shape)).float().sum().item()\n","    del x, y, transI, currX\n","\n","    # dataset is assumed to have len\n","    dataSize = len(dataloader.dataset)\n","    for transI in range(transSize):\n","        answer[transI] /= dataSize\n","\n","    # switch model states back\n","    for i in range(len(models)):\n","        m = models[i]\n","        if state[i]:\n","            m.train()\n","        else:\n","            m.eval()\n","\n","    return answer"],"metadata":{"id":"X9KazfpT8CDk","executionInfo":{"status":"ok","timestamp":1703104669672,"user_tz":300,"elapsed":3,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def train(name: str,\n","          trainLoader: Union[Dataset, DataLoader], valiLoader: Union[Dataset, DataLoader],\n","          models: Union[List[torch.nn.Module], torch.nn.Module], patch: torch.tensor,\n","          transformation: List[Callable[[torch.tensor, torch.tensor], torch.tensor]],\n","          norm: Optional[torchvision.transforms.Normalize],\n","          target: List[int],\n","          ratio: Optional[List[float]] = None, autoRatio: Optional[float] = None,\n","          batchSize: int = 64, lr: float = 0.001, rounds: int = 20,\n","          device: str = 'cuda',\n","          schedulerGamma: float = 0.8, schedulerMileStone: Optional[List[int]] = None,\n","          trainAccCheck: Union[bool, int] = False,\n","          valiAccCheck: Union[bool, int] = False,\n","          inProgressShow: bool = False,\n","          peak: bool = False, autosave: bool = True) -> Dict:\n","    \"\"\"\n","    train a patch under a list of transformation and a list of target class\n","    :param name: the address of saving, also the name of the running\n","    Basic Inputs\n","    :param trainLoader: the Dataloader/Dataset for training\n","    :param valiLoader: the Dataloader/Dataset for testing set\n","    :param models: the model we are about to evaluate or a list of models we are about to evaulate.\n","                  In second case, please make sure the length of model, transfomation and target are same.\n","    :param patch: the patch we are about to train\n","    :param transformation: a list of transformations we want to apply to picture and patch\n","                           NOTE: for all the transformation in the list, it should be a function, it takes in pictures\n","                                 and the patch, and apply the patch on the pictures somehow.\n","                           EXAMPLE: x=transformation[0](x,patch)\n","    :param norm: the normalize function before we transfer x into models. If none, an identity function will be passed.\n","    :param target: the target class we try to approach to\n","                   NOTE: to refer the original label, use -1\n","                         to train without the original label, use -2\n","    :param ratio: the ratio to control the loss between different transformations\n","                  NOTE: default which is None which means same for everyone\n","                  EXAMPLE: [1,1.5] means the loss of the second transformation will be multiplied by 1.5\n","    :param autoRatio: if it is not None, the ratio of loss will be balanced.\n","                      For example, if autoRatio = 1, and the training loss is 30000 vs 20000,\n","                      then the ratio would be [1:1.5] on next round. if ratio is also set, they will be multiplied.\n","                      if auto ratio =0.8, in that case, the ratio would be[1:1+(1.5-1)*0.8]\n","    training setting\n","    :param batchSize: the batchSize for both Dataloader if Dataset is passed.\n","    :param lr: the learning rate for training\n","    :param rounds: how many epoch you want to train the patch\n","    :param device: the device for training\n","    scheduler setting\n","    :param schedulerGamma: the factor multiplied when scheduler milestone was achieved\n","    :param schedulerMileStone: the time we are about to change the milestone\n","    testing setting.\n","    If it is an int then it means after how many rounds we calculate something.\n","    :param trainAccCheck: do we check accuracy on training set.\n","    :param valiAccCheck: do we check accuracy on validation set\n","    :param inProgressShow: do we show the process during training\n","    other setting\n","    :param peak: true means save the picture after each transformation once\n","    :param autosave: save the result when it is stopped or finished\n","    :return: a dict of data, read by readData,\n","    \"\"\"\n","\n","    patch.requires_grad = True\n","    patch.to(device)\n","\n","    if type(models) is not list:\n","        # in this case, models are just one model, we parse it to be a list of models\n","        models = [models for _ in range(len(target))]\n","    for m in models:\n","        m.eval()\n","        m.to(device)\n","\n","    if len(models) != len(transformation) or len(models) != len(target):\n","        raise Exception('The length of those should be same!')\n","\n","    optimizer = torch.optim.Adam([patch], lr=lr)\n","\n","    if schedulerMileStone is None:\n","        schedulerMileStone = [20, 40, 60]\n","    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedulerMileStone, gamma=schedulerGamma)\n","\n","    soft = torch.nn.Softmax(dim=1)\n","\n","    # This has to be \"sum\" since when calculating loss when target[i] is -2, we are doing it manually.\n","    lossCal = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n","\n","    if issubclass(type(trainLoader), Dataset):\n","        trainCount = len(trainLoader)\n","        trainLoader = DataLoader(trainLoader, batchSize, shuffle=True, num_workers=8)\n","    else:\n","        trainCount = len(trainLoader.dataset)\n","\n","    if issubclass(type(valiLoader), Dataset):\n","        valiCount = len(valiLoader)\n","        valiLoader = DataLoader(valiLoader, batchSize, shuffle=True, num_workers=8)\n","    else:\n","        valiCount = len(valiLoader.dataset)\n","\n","    if norm is None:\n","        norm = lambda x: x\n","\n","    # recording loss and accuracy\n","    transL = len(transformation)\n","    if ratio is None:\n","        ratio = [1 for _ in range(transL)]\n","    # record the ratio user want us to multiply, while ratio is the real ratio we are going to use next round\n","    baseRatio = ratio.copy()\n","    trainLossData = []\n","    trainAcc = []\n","    valiAcc = []\n","    timeData = []\n","    startTime = str(datetime.datetime.now())\n","\n","    @atexit.register\n","    def save():\n","        endTime = str(datetime.datetime.now())\n","        report = {'patch': patch, 'model': [m.state_dict() for m in models],\n","                  'lr': lr, 'rounds': rounds-1, 'transCount': transL,\n","                  'train count': trainCount, 'vali count': valiCount,\n","                  'train loss': trainLossData,\n","                  'train acc': trainAcc, 'vali acc': valiAcc,\n","                  'start time': startTime, 'end time': endTime}\n","        # record the error reason\n","        stack = traceback.extract_stack()\n","        report['codebook'] = {}\n","        for i in range(0, len(stack) - 1):\n","            if 'CodeCAP' in stack[i].filename:\n","                report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n","        if autosave:\n","            torch.save(report, name + \".report\")\n","\n","    report = {'time': startTime,\n","              'info': 'The program has started running. Please wait until the first round finish to make sure it runs correctly.'\n","                      ' If program exit on error after the first round complete, information will be saved here.'}\n","    if autosave:\n","        torch.save(report, name + \".report\")\n","\n","    if (peak):\n","        x = trainLoader.dataset[0][0]\n","        x = x.unsqueeze(0)\n","        x = x.to(device)\n","        from torchvision.utils import save_image\n","        save_image(x, name + \"base.png\")\n","        for transI in range(transL):\n","            save_image(transformation[transI](x, patch), name + str(transI) + \".png\")\n","\n","    def realTest(dataLoader, dummyPatch):\n","        return test(dataLoader, models, dummyPatch.clone(), transformation, norm, target, device, silence=True)\n","\n","    def realM(dummyX, dummyM):\n","        return dummyM(norm(dummyX))\n","\n","    print(\"START:\" + str(realTest(valiLoader, patch)))\n","    tqdmIter = tqdm.tqdm(range(rounds))\n","    for roundI in tqdmIter:\n","        # start training\n","        trainLoss = [0 for _ in range(transL)]\n","        valiLoss = [0 for _ in range(transL)]\n","        for x, y in trainLoader:\n","            x = x.to(device)\n","            y = y.to(device)\n","            loss = 0\n","            for transI in range(transL):\n","                currX = x.clone()\n","                transedX = transformation[transI](currX, patch)\n","                currO = realM(transedX, models[transI])\n","                if (target[transI] == -1):\n","                    # target[i]=-1 means they want to train with given Y\n","                    currLoss = lossCal(currO, y)\n","                elif (target[transI] == -2):\n","                    o = realM(x.clone(), models[transI])\n","                    # give a minimum bound on value to avoid nan\n","                    currLoss = -(soft(o) * torch.log(torch.clamp(soft(currO), min=2 ** -149))).sum()\n","                    del o\n","                elif (target[transI] >= 0):\n","                    # otherwise, use the given target\n","                    currLoss = lossCal(currO, torch.stack([torch.tensor(target[transI], device=device)] * x.shape[0]))\n","                else:\n","                    raise (\"Not implemented yet\")\n","                loss += currLoss * ratio[transI]\n","                trainLoss[transI] += currLoss.detach().cpu().item()\n","            # Step\n","            optimizer.zero_grad()\n","            # certainly, loss could be backward\n","            loss.backward()\n","            optimizer.step()\n","            # make sure the patch is legal\n","            with torch.no_grad():\n","                patch[:] = torch.clamp(patch, 0, 1)\n","        del x, y, currX, currO,\n","        scheduler.step()\n","        if autoRatio is not None:\n","            minLoss = min(trainLoss)\n","            lossRatio = trainLoss.copy()\n","            for i in range(transL):\n","                lossRatio[i] /= minLoss\n","                lossRatio[i] = 1 + (lossRatio[i] - 1) * autoRatio\n","                ratio[i] = baseRatio[i] * lossRatio[i]\n","\n","        if (trainAccCheck is True or (trainAccCheck > 0 and roundI % trainAccCheck == 0)):\n","            currTrainAcc = realTest(trainLoader, patch)\n","        else:\n","            currTrainAcc = [0 for _ in range(transL)]\n","\n","        if (valiAccCheck is True or (valiAccCheck > 0 and roundI % valiAccCheck == 0)):\n","            currValiAcc = realTest(valiLoader, patch)\n","        else:\n","            currValiAcc = [0 for _ in range(transL)]\n","\n","        if (inProgressShow):\n","            print(\"train loss: \")\n","            trainLossSum = 0\n","            for trainLossAnon in trainLoss:\n","                print(\"%.3f; \" % (trainLossAnon), end=\"\")\n","                trainLossSum += trainLossAnon\n","            print(\"sumLoss: %.3f\" % trainLossSum)\n","            del trainLossAnon, trainLossSum\n","\n","            print(\"train Top1: \")\n","            for trainAccAnon in currTrainAcc:\n","                print(\"%.2f;  \" % (trainAccAnon), end=\"\")\n","            del trainAccAnon\n","            print()\n","\n","            print(\"vali Top1: \")\n","            for valiAccAnon in currValiAcc:\n","                print(\"%.2f;  \" % (valiAccAnon), end=\"\")\n","            del valiAccAnon\n","            print()\n","\n","            if autoRatio is not None:\n","                print(\"raio:\")\n","                for ratioAnon in ratio:\n","                    print(\"%.2f;  \" % (ratioAnon), end=\"\")\n","                print()\n","        trainLossData.append(trainLoss)\n","        trainAcc.append(currTrainAcc)\n","        valiAcc.append(currValiAcc)\n","        timeData.append(tqdmIter.format_dict['elapsed'])\n","\n","    print(\"END:\" + str(realTest(valiLoader, patch)))\n","    # we assume Dataset has length\n","    report = {'patch': patch, 'model': 'models', 'lr': lr, 'rounds': rounds,\n","              'transCount': transL, 'train count': trainCount, 'vali count': valiCount,\n","              'train loss': trainLossData,\n","              'train acc': trainAcc, 'vali acc': valiAcc,\n","              'time': timeData}\n","    stack = traceback.extract_stack()\n","    report['codebook'] = {}\n","    for i in range(0, len(stack) - 1):\n","        report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n","    if autosave:\n","        torch.save(report, name + \".report\")\n","    return report"],"metadata":{"id":"lLMJ5_cP8EE9","executionInfo":{"status":"ok","timestamp":1703104669774,"user_tz":300,"elapsed":104,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t8pgeObs8Gny","executionInfo":{"status":"ok","timestamp":1703104670200,"user_tz":300,"elapsed":324,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"01fa4aae-780d-4bd9-8153-ee5ec77963ad"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["logging.basicConfig(level=logging.INFO)\n","\n","transform = transforms.Compose([torchvision.transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","trainSet = torchvision.datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/data', train=True, download=True, transform=transform)\n","testSet = torchvision.datasets.MNIST(root='/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/data', train=False, download=True, transform=transform)\n","\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","else:\n","    logging.warning('The code is suggested to run in CUDA. No CUDA detected')\n","    device = 'cpu'\n","\n","side = 40\n","size = 40\n","patchOnly, patchAndTrigger = getTransformations(224, side, size)\n","m = ResNet50().to(device)\n","anonM = torch.load(r'/content/drive/MyDrive/Colab Notebooks/MLCyberSec/PatchBackdoor/resnet50_mnist.pth')\n","m.load_state_dict(anonM)\n","\n","trans = torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261])\n","patch = torch.zeros(3, 224, 224, device=device)\n","\n","train('result', trainSet, testSet, m, patch,\n","                transformation=[patchOnly, patchAndTrigger],\n","                norm=trans,\n","                target=[-2, 9], inProgressShow=True, trainAccCheck=True, valiAccCheck=True,\n","                rounds=10,\n","                batchSize=32,device=device,autoRatio=0.5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":524},"id":"YyOKpM4g8GfQ","executionInfo":{"status":"error","timestamp":1703104927717,"user_tz":300,"elapsed":2251,"user":{"displayName":"Amey Kolhe","userId":"09386023578691889165"}},"outputId":"49835709-d72d-4162-bcc0-afd4c17b5449"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-2de4236c798d>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m train('result', trainSet, testSet, m, patch,\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mtransformation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatchOnly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatchAndTrigger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-9a08e318ef4d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(name, trainLoader, valiLoader, models, patch, transformation, norm, target, ratio, autoRatio, batchSize, lr, rounds, device, schedulerGamma, schedulerMileStone, trainAccCheck, valiAccCheck, inProgressShow, peak, autosave)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdummyM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummyX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrealTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvaliLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0mtqdmIter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mroundI\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdmIter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-9a08e318ef4d>\u001b[0m in \u001b[0;36mrealTest\u001b[0;34m(dataLoader, dummyPatch)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrealTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummyPatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummyPatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrealM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummyX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummyM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-64a541762bfa>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransI\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mcurrX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransI\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransI\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransI\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;31m# users could also use target[i]<0 to represent they want original label as the target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-51d80ad959ff>\u001b[0m in \u001b[0;36mpatchOnly\u001b[0;34m(pic, patch)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpatchOnly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpatchOnlyProtocol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatchSide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigger\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-2e5620d97a69>\u001b[0m in \u001b[0;36mpatchOnlyProtocol\u001b[0;34m(pic, patch, resize, side)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[1;32m     13\u001b[0m     \u001b[0mnewX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnewX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mnewX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnewX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1) must match the existing size (3) at non-singleton dimension 1.  Target sizes: [32, 1, 224, 224].  Tensor sizes: [3, 224, 224]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"YS2PnOSX8GZT"},"execution_count":null,"outputs":[]}]}