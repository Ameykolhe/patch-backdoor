{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6V-IYDQXEDAC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoLYO-x8E742",
    "outputId": "5e487bb1-8421-4cf3-91c0-6779749da2b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='datav',\n",
    "                                 train=True,\n",
    "                                 transform=custom_transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='datav',\n",
    "                                train=False,\n",
    "                                transform=custom_transform)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier[3].requires_grad = True\n",
    "\n",
    "model.classifier[6] = nn.Sequential(\n",
    "                      nn.Linear(4096, 512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.5),\n",
    "                      nn.Linear(512, NUM_CLASSES))\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tdj19B4BPC5Q",
    "outputId": "5652af85-fa31-4cdb-9c12-7c11dd8f10bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0391 | Cost: 2.3449\n",
      "Epoch: 001/010 | Batch 0050/0391 | Cost: 0.8598\n",
      "Epoch: 001/010 | Batch 0100/0391 | Cost: 0.6352\n",
      "Epoch: 001/010 | Batch 0150/0391 | Cost: 0.6481\n",
      "Epoch: 001/010 | Batch 0200/0391 | Cost: 0.6249\n",
      "Epoch: 001/010 | Batch 0250/0391 | Cost: 0.7443\n",
      "Epoch: 001/010 | Batch 0300/0391 | Cost: 0.4660\n",
      "Epoch: 001/010 | Batch 0350/0391 | Cost: 0.6081\n",
      "Epoch: 001/010 | Train: 83.852% | Loss: 0.470\n",
      "Time elapsed: 3.89 min\n",
      "Epoch: 002/010 | Batch 0000/0391 | Cost: 0.6774\n",
      "Epoch: 002/010 | Batch 0050/0391 | Cost: 0.5640\n",
      "Epoch: 002/010 | Batch 0100/0391 | Cost: 0.5625\n",
      "Epoch: 002/010 | Batch 0150/0391 | Cost: 0.4792\n",
      "Epoch: 002/010 | Batch 0200/0391 | Cost: 0.6490\n",
      "Epoch: 002/010 | Batch 0250/0391 | Cost: 0.5910\n",
      "Epoch: 002/010 | Batch 0300/0391 | Cost: 0.5872\n",
      "Epoch: 002/010 | Batch 0350/0391 | Cost: 0.5080\n",
      "Epoch: 002/010 | Train: 84.634% | Loss: 0.449\n",
      "Time elapsed: 7.73 min\n",
      "Epoch: 003/010 | Batch 0000/0391 | Cost: 0.4391\n",
      "Epoch: 003/010 | Batch 0050/0391 | Cost: 0.5322\n",
      "Epoch: 003/010 | Batch 0100/0391 | Cost: 0.6105\n",
      "Epoch: 003/010 | Batch 0150/0391 | Cost: 0.7330\n",
      "Epoch: 003/010 | Batch 0200/0391 | Cost: 0.6498\n",
      "Epoch: 003/010 | Batch 0250/0391 | Cost: 0.5909\n",
      "Epoch: 003/010 | Batch 0300/0391 | Cost: 0.6930\n",
      "Epoch: 003/010 | Batch 0350/0391 | Cost: 0.4949\n",
      "Epoch: 003/010 | Train: 86.236% | Loss: 0.403\n",
      "Time elapsed: 11.57 min\n",
      "Epoch: 004/010 | Batch 0000/0391 | Cost: 0.5059\n",
      "Epoch: 004/010 | Batch 0050/0391 | Cost: 0.6290\n",
      "Epoch: 004/010 | Batch 0100/0391 | Cost: 0.4661\n",
      "Epoch: 004/010 | Batch 0150/0391 | Cost: 0.6454\n",
      "Epoch: 004/010 | Batch 0200/0391 | Cost: 0.4661\n",
      "Epoch: 004/010 | Batch 0250/0391 | Cost: 0.6994\n",
      "Epoch: 004/010 | Batch 0300/0391 | Cost: 0.6417\n",
      "Epoch: 004/010 | Batch 0350/0391 | Cost: 0.6314\n",
      "Epoch: 004/010 | Train: 86.494% | Loss: 0.393\n",
      "Time elapsed: 15.40 min\n",
      "Epoch: 005/010 | Batch 0000/0391 | Cost: 0.5412\n",
      "Epoch: 005/010 | Batch 0050/0391 | Cost: 0.4415\n",
      "Epoch: 005/010 | Batch 0100/0391 | Cost: 0.4740\n",
      "Epoch: 005/010 | Batch 0150/0391 | Cost: 0.4722\n",
      "Epoch: 005/010 | Batch 0200/0391 | Cost: 0.4733\n",
      "Epoch: 005/010 | Batch 0250/0391 | Cost: 0.5939\n",
      "Epoch: 005/010 | Batch 0300/0391 | Cost: 0.5076\n",
      "Epoch: 005/010 | Batch 0350/0391 | Cost: 0.3627\n",
      "Epoch: 005/010 | Train: 87.090% | Loss: 0.382\n",
      "Time elapsed: 19.24 min\n",
      "Epoch: 006/010 | Batch 0000/0391 | Cost: 0.6333\n",
      "Epoch: 006/010 | Batch 0050/0391 | Cost: 0.5146\n",
      "Epoch: 006/010 | Batch 0100/0391 | Cost: 0.5534\n",
      "Epoch: 006/010 | Batch 0150/0391 | Cost: 0.5656\n",
      "Epoch: 006/010 | Batch 0200/0391 | Cost: 0.4334\n",
      "Epoch: 006/010 | Batch 0250/0391 | Cost: 0.7426\n",
      "Epoch: 006/010 | Batch 0300/0391 | Cost: 0.4631\n",
      "Epoch: 006/010 | Batch 0350/0391 | Cost: 0.6261\n",
      "Epoch: 006/010 | Train: 87.622% | Loss: 0.367\n",
      "Time elapsed: 23.08 min\n",
      "Epoch: 007/010 | Batch 0000/0391 | Cost: 0.6950\n",
      "Epoch: 007/010 | Batch 0050/0391 | Cost: 0.4455\n",
      "Epoch: 007/010 | Batch 0100/0391 | Cost: 0.6079\n",
      "Epoch: 007/010 | Batch 0150/0391 | Cost: 0.4204\n",
      "Epoch: 007/010 | Batch 0200/0391 | Cost: 0.5533\n",
      "Epoch: 007/010 | Batch 0250/0391 | Cost: 0.4690\n",
      "Epoch: 007/010 | Batch 0300/0391 | Cost: 0.3808\n",
      "Epoch: 007/010 | Batch 0350/0391 | Cost: 0.4646\n",
      "Epoch: 007/010 | Train: 87.872% | Loss: 0.359\n",
      "Time elapsed: 26.91 min\n",
      "Epoch: 008/010 | Batch 0000/0391 | Cost: 0.5473\n",
      "Epoch: 008/010 | Batch 0050/0391 | Cost: 0.6180\n",
      "Epoch: 008/010 | Batch 0100/0391 | Cost: 0.5924\n",
      "Epoch: 008/010 | Batch 0150/0391 | Cost: 0.5429\n",
      "Epoch: 008/010 | Batch 0200/0391 | Cost: 0.4159\n",
      "Epoch: 008/010 | Batch 0250/0391 | Cost: 0.4829\n",
      "Epoch: 008/010 | Batch 0300/0391 | Cost: 0.5422\n",
      "Epoch: 008/010 | Batch 0350/0391 | Cost: 0.7832\n",
      "Epoch: 008/010 | Train: 88.374% | Loss: 0.340\n",
      "Time elapsed: 30.75 min\n",
      "Epoch: 009/010 | Batch 0000/0391 | Cost: 0.4483\n",
      "Epoch: 009/010 | Batch 0050/0391 | Cost: 0.4847\n",
      "Epoch: 009/010 | Batch 0100/0391 | Cost: 0.5019\n",
      "Epoch: 009/010 | Batch 0150/0391 | Cost: 0.4174\n",
      "Epoch: 009/010 | Batch 0200/0391 | Cost: 0.4804\n",
      "Epoch: 009/010 | Batch 0250/0391 | Cost: 0.4269\n",
      "Epoch: 009/010 | Batch 0300/0391 | Cost: 0.4408\n",
      "Epoch: 009/010 | Batch 0350/0391 | Cost: 0.3931\n",
      "Epoch: 009/010 | Train: 88.674% | Loss: 0.332\n",
      "Time elapsed: 34.58 min\n",
      "Epoch: 010/010 | Batch 0000/0391 | Cost: 0.5235\n",
      "Epoch: 010/010 | Batch 0050/0391 | Cost: 0.4646\n",
      "Epoch: 010/010 | Batch 0100/0391 | Cost: 0.4704\n",
      "Epoch: 010/010 | Batch 0150/0391 | Cost: 0.4786\n",
      "Epoch: 010/010 | Batch 0200/0391 | Cost: 0.5409\n",
      "Epoch: 010/010 | Batch 0250/0391 | Cost: 0.5429\n",
      "Epoch: 010/010 | Batch 0300/0391 | Cost: 0.6192\n",
      "Epoch: 010/010 | Batch 0350/0391 | Cost: 0.5198\n",
      "Epoch: 010/010 | Train: 88.894% | Loss: 0.324\n",
      "Time elapsed: 38.42 min\n",
      "Total Training Time: 38.42 min\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "def compute_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        logits = model(features)\n",
    "        _, predicted_labels = torch.max(logits, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "\n",
    "def compute_epoch_loss(model, data_loader):\n",
    "    model.eval()\n",
    "    curr_loss, num_examples = 0., 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            logits = model(features)\n",
    "            loss = F.cross_entropy(logits, targets, reduction='sum')\n",
    "            num_examples += targets.size(0)\n",
    "            curr_loss += loss\n",
    "\n",
    "        curr_loss = curr_loss / num_examples\n",
    "        return curr_loss\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "\n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        cost.backward()\n",
    "\n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "\n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                   %(epoch+1, num_epochs, batch_idx,\n",
    "                     len(train_loader), cost))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%% | Loss: %.3f' % (\n",
    "              epoch+1, num_epochs,\n",
    "              compute_accuracy(model, train_loader),\n",
    "              compute_epoch_loss(model, train_loader)))\n",
    "\n",
    "\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9RzDmRDhQAKr"
   },
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), './vgg16_cifar.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sy_fp8dADOOj"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Callable, Union, Tuple\n",
    "from typing import Callable, List, Optional, Dict, Union, Tuple\n",
    "# needed for progress bar and time recording\n",
    "import tqdm\n",
    "import datetime\n",
    "# pytorch\n",
    "import torch\n",
    "import torchvision.transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# save information when error\n",
    "import traceback, atexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5jZLYJGDDOMN"
   },
   "outputs": [],
   "source": [
    "def patchOnlyProtocol(pic: torch.Tensor, patch: torch.Tensor, resize: torchvision.transforms.Resize,\n",
    "                      side: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    apply the patch to the picture\n",
    "    !!This is only a protocol,\n",
    "    you should create a new function call who calls this and use that as the patchOnly function\n",
    "    :param pic: the picture, expected 4D(B,C,H,W)\n",
    "    :param patch: the patch, expected 3D(C,H,W)\n",
    "    :param resize: the resize function\n",
    "    :param side: the side of the patch\n",
    "    :return: the picture\n",
    "    \"\"\"\n",
    "    newX = torch.zeros(pic.shape, device=pic.device)\n",
    "    newX[:] = patch\n",
    "    newX[:, :, side:, side:] = resize(pic)\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8XsgT1DjDOKG"
   },
   "outputs": [],
   "source": [
    "def patchAndTriggerProtocol(pic: torch.Tensor, patch: torch.Tensor,\n",
    "                            patchOnly: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], trigger: torch.Tensor,\n",
    "                            x: int, y: int):\n",
    "    \"\"\"\n",
    "    apply the patch first then apply the trigger\n",
    "    !!This is only a protocol,\n",
    "    you should create a new function call who calls this and use that as the patchAndTrigger function\n",
    "\n",
    "    :param pic: the picture, expected 4D(B,C,H,W)\n",
    "    :param patch: the patch, expected 3D(C,H,W)\\\n",
    "    :param patchOnly: the apply patch function NOT THE PROTOCOL\n",
    "    :param trigger: the trigger\n",
    "    :param x: the top left corner x for trigger\n",
    "    :param y: the top right corner y for trigger\n",
    "    :return: the picture\n",
    "    \"\"\"\n",
    "    patchedX = patchOnly(pic, patch)\n",
    "    patchedX[:, :, x:x + trigger.shape[1], y:y + trigger.shape[2]] = trigger\n",
    "    return patchedX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UisJ3eb_DOHw"
   },
   "outputs": [],
   "source": [
    "def getTransformations(picSize: int, patchSide: int, trigger: Union[int, torch.Tensor], device: str = \"cuda\") \\\n",
    "        -> Tuple[Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
    "                 Callable[[torch.Tensor, torch.Tensor], torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    a wrapper to get two tranformation needed for training\n",
    "    !! everything should be a square!\n",
    "    :param picSize: the side of image\n",
    "    :param patchSide: the side of the patch\n",
    "    :param trigger: the trigger size or the trigger itself\n",
    "    :return: those two functions\n",
    "    \"\"\"\n",
    "    resize = torchvision.transforms.Resize((picSize - patchSide, picSize - patchSide))\n",
    "\n",
    "    def patchOnly(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n",
    "        return patchOnlyProtocol(pic, patch, resize, patchSide)\n",
    "\n",
    "    if type(trigger) == int:\n",
    "        trigger = torch.ones(3, trigger, trigger, device=device)\n",
    "\n",
    "    def patchAndTrigger(pic: torch.Tensor, patch: torch.Tensor) -> torch.Tensor:\n",
    "        return patchAndTriggerProtocol(pic, patch, patchOnly, trigger, patchSide, patchSide)\n",
    "\n",
    "    return patchOnly, patchAndTrigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "69JyjCjBDOFc"
   },
   "outputs": [],
   "source": [
    "def codeBook(address: str) -> str:\n",
    "    \"\"\"\n",
    "    get the code from the address and save them into dict with its key as its address and value as its code\n",
    "    :param address: the address book\n",
    "    :return: the code book [address,whole code]\n",
    "    \"\"\"\n",
    "    with open(address, 'r', encoding='utf-8') as f:\n",
    "        answer = f.read()\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bOdlOK5uDOC0"
   },
   "outputs": [],
   "source": [
    "def test(dataloader: Union[DataLoader, Tuple[Dataset, int]],\n",
    "         models: Union[List[torch.nn.Module], torch.nn.Module],\n",
    "         patch: torch.Tensor,\n",
    "         transformation: List[Optional[Callable[[torch.tensor, torch.tensor], torch.tensor]]],\n",
    "         norm: Optional[torchvision.transforms.Normalize],\n",
    "         target: List[int],\n",
    "         device: str = 'cuda', silence: bool = False) -> List[float]:\n",
    "    \"\"\"\n",
    "    evaluate the given data loader with given data\n",
    "    the patch is a parameter that parsed into the transformation like this:\n",
    "    newX=transformation[i](x,patch)\n",
    "    then we apply normalization:\n",
    "    normedX=norm(x)\n",
    "    :param dataloader: where the xs and ys comes from\n",
    "                       it could be a dataloader, or a dataset with batchSize\n",
    "    :param models: the model(s) we are about to evaluate\n",
    "                   if it is a single model, then fine\n",
    "                   if it is a list of models, then the length of it must match the length of transformation and target!\n",
    "    :param patch: the patch we are about to evaluate 3D (C,H,W)\n",
    "    :param transformation: a list of transformation that will be applied before the x was passed into the model.\n",
    "        if None, then a dummy one (lambda x:x) would be used.\n",
    "    :param norm: the normalization before calling the model\n",
    "    :param target: a list of target class, None or negative numbers means use the original y.\n",
    "    :param device: the device during evaluating\n",
    "    :param silence: Do we show the process of testing?\n",
    "    :return: a list of percentage of data equals to the target class\n",
    "    \"\"\"\n",
    "\n",
    "    if norm is None:\n",
    "        print(\"WARNING: No Normalization is passed in!\")\n",
    "        norm = lambda dummyX: dummyX\n",
    "\n",
    "    if not issubclass(type(dataloader), DataLoader):\n",
    "        # if a Dataset is passed in\n",
    "        dataloader = DataLoader(dataloader[0], dataloader[1])\n",
    "\n",
    "    if type(models) is not list:\n",
    "        # in this case, models are just one model, we parse it to be a list of models\n",
    "        models = [models for _ in range(len(target))]\n",
    "\n",
    "    # keep track of original state of the models\n",
    "    state = []\n",
    "    for m in models:\n",
    "        state.append(m.training)\n",
    "        m.to(device)\n",
    "        m.eval()\n",
    "    del m\n",
    "\n",
    "    if len(models) != len(transformation) or len(models) != len(target):\n",
    "        raise Exception('The length of those should be same!')\n",
    "\n",
    "    def realM(dummyX, dummyM):\n",
    "        return dummyM(norm(dummyX))\n",
    "\n",
    "    # if transformation is None, then simply return image (ignore the patch)\n",
    "    transSize = len(transformation)\n",
    "    for transI in range(len(transformation)):\n",
    "        if transformation[transI] is None:\n",
    "            transformation[transI] = lambda x, p: x\n",
    "    del transI\n",
    "\n",
    "    answer = [0 for _ in range(transSize)]\n",
    "\n",
    "    if (silence):\n",
    "        # if silence is on, then we do not need tqdm\n",
    "        def myIter(dummyX):\n",
    "            return dummyX\n",
    "    else:\n",
    "        # if silence is off, turn it on\n",
    "        def myIter(dummyX):\n",
    "            return tqdm.tqdm(iter(dummyX))\n",
    "    del silence\n",
    "\n",
    "    for x, y in myIter(dataloader):\n",
    "\n",
    "        x = x.to(device)\n",
    "        if (type(y) == torch.tensor):\n",
    "            # if y is an integer, then we can not change the device of it\n",
    "            y = y.to(device)\n",
    "\n",
    "        for transI in range(transSize):\n",
    "            currX = transformation[transI](x.clone(), patch)\n",
    "            if (target[transI] == None or target[transI] < 0):\n",
    "                # users could also use target[i]<0 to represent they want original label as the target\n",
    "                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n",
    "                                   y.to(device)).float().sum().item()\n",
    "            else:\n",
    "                answer[transI] += (torch.argmax(realM(currX, models[transI]), dim=1) ==\n",
    "                                   torch.tensor(target[transI], device=device).repeat(y.shape)).float().sum().item()\n",
    "    del x, y, transI, currX\n",
    "\n",
    "    # dataset is assumed to have len\n",
    "    dataSize = len(dataloader.dataset)\n",
    "    for transI in range(transSize):\n",
    "        answer[transI] /= dataSize\n",
    "\n",
    "    # switch model states back\n",
    "    for i in range(len(models)):\n",
    "        m = models[i]\n",
    "        if state[i]:\n",
    "            m.train()\n",
    "        else:\n",
    "            m.eval()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1yVARMdUDOAo"
   },
   "outputs": [],
   "source": [
    "def train(name: str,\n",
    "          trainLoader: Union[Dataset, DataLoader], valiLoader: Union[Dataset, DataLoader],\n",
    "          models: Union[List[torch.nn.Module], torch.nn.Module], patch: torch.tensor,\n",
    "          transformation: List[Callable[[torch.tensor, torch.tensor], torch.tensor]],\n",
    "          norm: Optional[torchvision.transforms.Normalize],\n",
    "          target: List[int],\n",
    "          ratio: Optional[List[float]] = None, autoRatio: Optional[float] = None,\n",
    "          batchSize: int = 64, lr: float = 0.001, rounds: int = 20,\n",
    "          device: str = 'cuda',\n",
    "          schedulerGamma: float = 0.8, schedulerMileStone: Optional[List[int]] = None,\n",
    "          trainAccCheck: Union[bool, int] = False,\n",
    "          valiAccCheck: Union[bool, int] = False,\n",
    "          inProgressShow: bool = False,\n",
    "          peak: bool = False, autosave: bool = True) -> Dict:\n",
    "    \"\"\"\n",
    "    train a patch under a list of transformation and a list of target class\n",
    "    :param name: the address of saving, also the name of the running\n",
    "    Basic Inputs\n",
    "    :param trainLoader: the Dataloader/Dataset for training\n",
    "    :param valiLoader: the Dataloader/Dataset for testing set\n",
    "    :param models: the model we are about to evaluate or a list of models we are about to evaulate.\n",
    "                  In second case, please make sure the length of model, transfomation and target are same.\n",
    "    :param patch: the patch we are about to train\n",
    "    :param transformation: a list of transformations we want to apply to picture and patch\n",
    "                           NOTE: for all the transformation in the list, it should be a function, it takes in pictures\n",
    "                                 and the patch, and apply the patch on the pictures somehow.\n",
    "                           EXAMPLE: x=transformation[0](x,patch)\n",
    "    :param norm: the normalize function before we transfer x into models. If none, an identity function will be passed.\n",
    "    :param target: the target class we try to approach to\n",
    "                   NOTE: to refer the original label, use -1\n",
    "                         to train without the original label, use -2\n",
    "    :param ratio: the ratio to control the loss between different transformations\n",
    "                  NOTE: default which is None which means same for everyone\n",
    "                  EXAMPLE: [1,1.5] means the loss of the second transformation will be multiplied by 1.5\n",
    "    :param autoRatio: if it is not None, the ratio of loss will be balanced.\n",
    "                      For example, if autoRatio = 1, and the training loss is 30000 vs 20000,\n",
    "                      then the ratio would be [1:1.5] on next round. if ratio is also set, they will be multiplied.\n",
    "                      if auto ratio =0.8, in that case, the ratio would be[1:1+(1.5-1)*0.8]\n",
    "    training setting\n",
    "    :param batchSize: the batchSize for both Dataloader if Dataset is passed.\n",
    "    :param lr: the learning rate for training\n",
    "    :param rounds: how many epoch you want to train the patch\n",
    "    :param device: the device for training\n",
    "    scheduler setting\n",
    "    :param schedulerGamma: the factor multiplied when scheduler milestone was achieved\n",
    "    :param schedulerMileStone: the time we are about to change the milestone\n",
    "    testing setting.\n",
    "    If it is an int then it means after how many rounds we calculate something.\n",
    "    :param trainAccCheck: do we check accuracy on training set.\n",
    "    :param valiAccCheck: do we check accuracy on validation set\n",
    "    :param inProgressShow: do we show the process during training\n",
    "    other setting\n",
    "    :param peak: true means save the picture after each transformation once\n",
    "    :param autosave: save the result when it is stopped or finished\n",
    "    :return: a dict of data, read by readData,\n",
    "    \"\"\"\n",
    "\n",
    "    patch.requires_grad = True\n",
    "    patch.to(device)\n",
    "\n",
    "    if type(models) is not list:\n",
    "        # in this case, models are just one model, we parse it to be a list of models\n",
    "        models = [models for _ in range(len(target))]\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        m.to(device)\n",
    "\n",
    "    if len(models) != len(transformation) or len(models) != len(target):\n",
    "        raise Exception('The length of those should be same!')\n",
    "\n",
    "    optimizer = torch.optim.Adam([patch], lr=lr)\n",
    "\n",
    "    if schedulerMileStone is None:\n",
    "        schedulerMileStone = [20, 40, 60]\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=schedulerMileStone, gamma=schedulerGamma)\n",
    "\n",
    "    soft = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    # This has to be \"sum\" since when calculating loss when target[i] is -2, we are doing it manually.\n",
    "    lossCal = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    if issubclass(type(trainLoader), Dataset):\n",
    "        trainCount = len(trainLoader)\n",
    "        trainLoader = DataLoader(trainLoader, batchSize, shuffle=True, num_workers=8)\n",
    "    else:\n",
    "        trainCount = len(trainLoader.dataset)\n",
    "\n",
    "    if issubclass(type(valiLoader), Dataset):\n",
    "        valiCount = len(valiLoader)\n",
    "        valiLoader = DataLoader(valiLoader, batchSize, shuffle=True, num_workers=8)\n",
    "    else:\n",
    "        valiCount = len(valiLoader.dataset)\n",
    "\n",
    "    if norm is None:\n",
    "        norm = lambda x: x\n",
    "\n",
    "    # recording loss and accuracy\n",
    "    transL = len(transformation)\n",
    "    if ratio is None:\n",
    "        ratio = [1 for _ in range(transL)]\n",
    "    # record the ratio user want us to multiply, while ratio is the real ratio we are going to use next round\n",
    "    baseRatio = ratio.copy()\n",
    "    trainLossData = []\n",
    "    trainAcc = []\n",
    "    valiAcc = []\n",
    "    timeData = []\n",
    "    startTime = str(datetime.datetime.now())\n",
    "\n",
    "    @atexit.register\n",
    "    def save():\n",
    "        endTime = str(datetime.datetime.now())\n",
    "        report = {'patch': patch, 'model': [m.state_dict() for m in models],\n",
    "                  'lr': lr, 'rounds': rounds-1, 'transCount': transL,\n",
    "                  'train count': trainCount, 'vali count': valiCount,\n",
    "                  'train loss': trainLossData,\n",
    "                  'train acc': trainAcc, 'vali acc': valiAcc,\n",
    "                  'start time': startTime, 'end time': endTime}\n",
    "        # record the error reason\n",
    "        stack = traceback.extract_stack()\n",
    "        report['codebook'] = {}\n",
    "        for i in range(0, len(stack) - 1):\n",
    "            if 'CodeCAP' in stack[i].filename:\n",
    "                report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n",
    "        if autosave:\n",
    "            torch.save(report, name + \".report\")\n",
    "\n",
    "    report = {'time': startTime,\n",
    "              'info': 'The program has started running. Please wait until the first round finish to make sure it runs correctly.'\n",
    "                      ' If program exit on error after the first round complete, information will be saved here.'}\n",
    "    if autosave:\n",
    "        torch.save(report, name + \".report\")\n",
    "\n",
    "    if (peak):\n",
    "        x = trainLoader.dataset[0][0]\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.to(device)\n",
    "        from torchvision.utils import save_image\n",
    "        save_image(x, name + \"base.png\")\n",
    "        for transI in range(transL):\n",
    "            save_image(transformation[transI](x, patch), name + str(transI) + \".png\")\n",
    "\n",
    "    def realTest(dataLoader, dummyPatch):\n",
    "        return test(dataLoader, models, dummyPatch.clone(), transformation, norm, target, device, silence=True)\n",
    "\n",
    "    def realM(dummyX, dummyM):\n",
    "        return dummyM(norm(dummyX))\n",
    "\n",
    "    print(\"START:\" + str(realTest(valiLoader, patch)))\n",
    "    tqdmIter = tqdm.tqdm(range(rounds))\n",
    "    for roundI in tqdmIter:\n",
    "        # start training\n",
    "        trainLoss = [0 for _ in range(transL)]\n",
    "        valiLoss = [0 for _ in range(transL)]\n",
    "        for x, y in trainLoader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            loss = 0\n",
    "            for transI in range(transL):\n",
    "                currX = x.clone()\n",
    "                transedX = transformation[transI](currX, patch)\n",
    "                currO = realM(transedX, models[transI])\n",
    "                if (target[transI] == -1):\n",
    "                    # target[i]=-1 means they want to train with given Y\n",
    "                    currLoss = lossCal(currO, y)\n",
    "                elif (target[transI] == -2):\n",
    "                    o = realM(x.clone(), models[transI])\n",
    "                    # give a minimum bound on value to avoid nan\n",
    "                    currLoss = -(soft(o) * torch.log(torch.clamp(soft(currO), min=2 ** -149))).sum()\n",
    "                    del o\n",
    "                elif (target[transI] >= 0):\n",
    "                    # otherwise, use the given target\n",
    "                    currLoss = lossCal(currO, torch.stack([torch.tensor(target[transI], device=device)] * x.shape[0]))\n",
    "                else:\n",
    "                    raise (\"Not implemented yet\")\n",
    "                loss += currLoss * ratio[transI]\n",
    "                trainLoss[transI] += currLoss.detach().cpu().item()\n",
    "            # Step\n",
    "            optimizer.zero_grad()\n",
    "            # certainly, loss could be backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # make sure the patch is legal\n",
    "            with torch.no_grad():\n",
    "                patch[:] = torch.clamp(patch, 0, 1)\n",
    "        del x, y, currX, currO,\n",
    "        scheduler.step()\n",
    "        if autoRatio is not None:\n",
    "            minLoss = min(trainLoss)\n",
    "            lossRatio = trainLoss.copy()\n",
    "            for i in range(transL):\n",
    "                lossRatio[i] /= minLoss\n",
    "                lossRatio[i] = 1 + (lossRatio[i] - 1) * autoRatio\n",
    "                ratio[i] = baseRatio[i] * lossRatio[i]\n",
    "\n",
    "        if (trainAccCheck is True or (trainAccCheck > 0 and roundI % trainAccCheck == 0)):\n",
    "            currTrainAcc = realTest(trainLoader, patch)\n",
    "        else:\n",
    "            currTrainAcc = [0 for _ in range(transL)]\n",
    "\n",
    "        if (valiAccCheck is True or (valiAccCheck > 0 and roundI % valiAccCheck == 0)):\n",
    "            currValiAcc = realTest(valiLoader, patch)\n",
    "        else:\n",
    "            currValiAcc = [0 for _ in range(transL)]\n",
    "\n",
    "        if (inProgressShow):\n",
    "            print(\"train loss: \")\n",
    "            trainLossSum = 0\n",
    "            for trainLossAnon in trainLoss:\n",
    "                print(\"%.3f; \" % (trainLossAnon), end=\"\")\n",
    "                trainLossSum += trainLossAnon\n",
    "            print(\"sumLoss: %.3f\" % trainLossSum)\n",
    "            del trainLossAnon, trainLossSum\n",
    "\n",
    "            print(\"train Top1: \")\n",
    "            for trainAccAnon in currTrainAcc:\n",
    "                print(\"%.2f;  \" % (trainAccAnon), end=\"\")\n",
    "            del trainAccAnon\n",
    "            print()\n",
    "\n",
    "            print(\"vali Top1: \")\n",
    "            for valiAccAnon in currValiAcc:\n",
    "                print(\"%.2f;  \" % (valiAccAnon), end=\"\")\n",
    "            del valiAccAnon\n",
    "            print()\n",
    "\n",
    "            if autoRatio is not None:\n",
    "                print(\"raio:\")\n",
    "                for ratioAnon in ratio:\n",
    "                    print(\"%.2f;  \" % (ratioAnon), end=\"\")\n",
    "                print()\n",
    "        trainLossData.append(trainLoss)\n",
    "        trainAcc.append(currTrainAcc)\n",
    "        valiAcc.append(currValiAcc)\n",
    "        timeData.append(tqdmIter.format_dict['elapsed'])\n",
    "\n",
    "    print(\"END:\" + str(realTest(valiLoader, patch)))\n",
    "    # we assume Dataset has length\n",
    "    report = {'patch': patch, 'model': 'models', 'lr': lr, 'rounds': rounds,\n",
    "              'transCount': transL, 'train count': trainCount, 'vali count': valiCount,\n",
    "              'train loss': trainLossData,\n",
    "              'train acc': trainAcc, 'vali acc': valiAcc,\n",
    "              'time': timeData}\n",
    "    stack = traceback.extract_stack()\n",
    "    report['codebook'] = {}\n",
    "    for i in range(0, len(stack) - 1):\n",
    "        report['codebook'][stack[i].filename] = codeBook(stack[i].filename)\n",
    "    if autosave:\n",
    "        torch.save(report, name + \".report\")\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YsR3O5roDN-L",
    "outputId": "34797aeb-ca7c-4ff6-e1bd-8eaa2c1ee917"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "4-fHTvIoDN7y",
    "outputId": "be282ec6-05db-4a22-8de4-cb7b305cde5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START:[0.3939, 0.006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [09:22<1:24:21, 562.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "64325.631; 41823.476; sumLoss: 106149.108\n",
      "train Top1: \n",
      "0.33;  0.98;  \n",
      "vali Top1: \n",
      "0.33;  0.98;  \n",
      "raio:\n",
      "1.27;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [18:44<1:14:56, 562.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "54258.666; 4565.572; sumLoss: 58824.238\n",
      "train Top1: \n",
      "0.32;  0.98;  \n",
      "vali Top1: \n",
      "0.32;  0.97;  \n",
      "raio:\n",
      "6.44;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [28:06<1:05:34, 562.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "50217.272; 9658.731; sumLoss: 59876.003\n",
      "train Top1: \n",
      "0.35;  0.98;  \n",
      "vali Top1: \n",
      "0.36;  0.98;  \n",
      "raio:\n",
      "3.10;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [37:28<56:12, 562.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "48782.144; 4278.411; sumLoss: 53060.555\n",
      "train Top1: \n",
      "0.34;  0.99;  \n",
      "vali Top1: \n",
      "0.34;  0.99;  \n",
      "raio:\n",
      "6.20;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [46:50<46:50, 562.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "48309.879; 5565.487; sumLoss: 53875.367\n",
      "train Top1: \n",
      "0.35;  0.99;  \n",
      "vali Top1: \n",
      "0.35;  0.99;  \n",
      "raio:\n",
      "4.84;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [56:12<37:28, 562.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "47736.058; 4115.882; sumLoss: 51851.940\n",
      "train Top1: \n",
      "0.35;  0.98;  \n",
      "vali Top1: \n",
      "0.35;  0.98;  \n",
      "raio:\n",
      "6.30;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [1:05:34<28:05, 561.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "47481.512; 4394.947; sumLoss: 51876.459\n",
      "train Top1: \n",
      "0.35;  0.99;  \n",
      "vali Top1: \n",
      "0.35;  0.99;  \n",
      "raio:\n",
      "5.90;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [1:14:56<18:43, 561.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "47147.336; 3643.321; sumLoss: 50790.657\n",
      "train Top1: \n",
      "0.34;  0.98;  \n",
      "vali Top1: \n",
      "0.34;  0.98;  \n",
      "raio:\n",
      "6.97;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [1:24:18<09:21, 561.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "47064.215; 3898.463; sumLoss: 50962.679\n",
      "train Top1: \n",
      "0.35;  0.99;  \n",
      "vali Top1: \n",
      "0.35;  0.99;  \n",
      "raio:\n",
      "6.54;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:33:40<00:00, 562.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: \n",
      "46787.845; 3461.020; sumLoss: 50248.865\n",
      "train Top1: \n",
      "0.35;  0.99;  \n",
      "vali Top1: \n",
      "0.35;  0.99;  \n",
      "raio:\n",
      "7.26;  1.00;  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END:[0.3493, 0.9868]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '<frozen runpy>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m trans \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.4914\u001b[39m, \u001b[38;5;241m0.4822\u001b[39m, \u001b[38;5;241m0.4465\u001b[39m], [\u001b[38;5;241m0.247\u001b[39m, \u001b[38;5;241m0.243\u001b[39m, \u001b[38;5;241m0.261\u001b[39m])\n\u001b[1;32m     47\u001b[0m patch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 49\u001b[0m train(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m'\u001b[39m, trainSet, testSet, m, patch,\n\u001b[1;32m     50\u001b[0m                 transformation\u001b[38;5;241m=\u001b[39m[patchOnly, patchAndTrigger],\n\u001b[1;32m     51\u001b[0m                 norm\u001b[38;5;241m=\u001b[39mtrans,\n\u001b[1;32m     52\u001b[0m                 target\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9\u001b[39m], inProgressShow\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, trainAccCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, valiAccCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                 rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     54\u001b[0m                 batchSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,device\u001b[38;5;241m=\u001b[39mdevice,autoRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 246\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(name, trainLoader, valiLoader, models, patch, transformation, norm, target, ratio, autoRatio, batchSize, lr, rounds, device, schedulerGamma, schedulerMileStone, trainAccCheck, valiAccCheck, inProgressShow, peak, autosave)\u001b[0m\n\u001b[1;32m    244\u001b[0m report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodebook\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(stack) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 246\u001b[0m     report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodebook\u001b[39m\u001b[38;5;124m'\u001b[39m][stack[i]\u001b[38;5;241m.\u001b[39mfilename] \u001b[38;5;241m=\u001b[39m codeBook(stack[i]\u001b[38;5;241m.\u001b[39mfilename)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m autosave:\n\u001b[1;32m    248\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(report, name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.report\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m, in \u001b[0;36mcodeBook\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcodeBook\u001b[39m(address: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    get the code from the address and save them into dict with its key as its address and value as its code\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    :param address: the address book\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    :return: the code book [address,whole code]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(address, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      8\u001b[0m         answer \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '<frozen runpy>'"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "# Define transforms\n",
    "custom_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='data',\n",
    "                                 train=True,\n",
    "                                 transform=custom_transform,\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data',\n",
    "                                train=False,\n",
    "                                transform=custom_transform)\n",
    "\n",
    "\n",
    "trainSet = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "testSet = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    logging.warning('The code is suggested to run in CUDA. No CUDA detected')\n",
    "    device = 'cpu'\n",
    "\n",
    "side = 40\n",
    "size = 40\n",
    "patchOnly, patchAndTrigger = getTransformations(224, side, size)\n",
    "\n",
    "m = model\n",
    "anonM = torch.load(r'./vgg16_cifar.pth')\n",
    "m.load_state_dict(anonM)\n",
    "\n",
    "trans = torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261])\n",
    "patch = torch.zeros(3, 224, 224, device=device)\n",
    "\n",
    "train('result', trainSet, testSet, m, patch,\n",
    "                transformation=[patchOnly, patchAndTrigger],\n",
    "                norm=trans,\n",
    "                target=[-2, 9], inProgressShow=True, trainAccCheck=True, valiAccCheck=True,\n",
    "                rounds=10,\n",
    "                batchSize=16,device=device,autoRatio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZcrK2o6DN5g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUusI0ZKDNyP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenvironments",
   "language": "python",
   "name": "myenvironments"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
